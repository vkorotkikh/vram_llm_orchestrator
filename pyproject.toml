[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "vram-llm-orchestrator"
version = "0.1.0"
description = "Backend-only multi-GPU VRAM-aware loader for llama.cpp/llama-cpp-python"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
  {name = "POC Generator"}
]
dependencies = [
  "fastapi>=0.111.0",
  "uvicorn[standard]>=0.30.0",
  "pydantic>=2.7.0",
  "httpx>=0.27.0",
  "nvidia-ml-py>=12.0.0",
  "llama-cpp-python>=0.3.0",
]

[project.scripts]
vram-llm = "vram_llm.cli:main"

[tool.setuptools]
packages = ["vram_llm", "vram_llm.engines"]
